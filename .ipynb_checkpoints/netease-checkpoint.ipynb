{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.240 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'dict'> <type 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[13,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#_*_coding:utf-8_*_\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import codecs\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import jieba\n",
    "jieba.initialize()\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "\n",
    "#     string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "#     string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "#     string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "#     string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "#     string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "#     string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "#     string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "#     string = re.sub(r\",\", \" , \", string)\n",
    "#     string = re.sub(r\"!\", \" ! \", string)\n",
    "#     string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "#     string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "#     string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "#     string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "#     return string.strip().lower()\n",
    "    string = re.sub(r\"！\", u\"！\", string)\n",
    "    string = re.sub(r\"\\?\", u\"？\", string)\n",
    "    string = re.sub(r\"\\.\", u\"。\", string)\n",
    "    string = re.sub(r\",\", u\"，\", string)\n",
    "    string = re.sub(ur\"[^\\u4e00-\\u9fa5，。？！]\",'',string)\n",
    "    return string\n",
    "\n",
    "\n",
    "def load_data_and_labels():\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    positive_examples = list(codecs.open(\"./data/comment.pos\",encoding='utf-8').readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = list(codecs.open(\"./data/comment.neg\",encoding='utf-8').readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    x_text = [jieba.lcut(s) for s in x_text]\n",
    "    # Generate labels\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text, y]\n",
    "\n",
    "\n",
    "def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences\n",
    "\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]\n",
    "\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentencs and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    return [x, y]\n",
    "\n",
    "def build_predict_data(sentence,vocabulary,sentence_max_len):\n",
    "\n",
    "    if len(sentence)<sentence_max_len:\n",
    "        sentence.extend(['<PAD/>']*(sentence_max_len-len(sentence)))\n",
    "  \n",
    "    x=[]\n",
    "    for word in sentence:\n",
    "        if word in vocabulary.keys():\n",
    "            x.append(vocabulary[word])\n",
    "        else:\n",
    "            x.append(vocabulary['<PAD/>'])\n",
    "    x=[x]\n",
    "    return np.asarray(x)\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads and preprocessed data for the MR dataset.\n",
    "    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    sentences, labels = load_data_and_labels()\n",
    "    sentences_padded = pad_sentences(sentences)\n",
    "    vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "    x, y = build_input_data(sentences_padded, labels, vocabulary)\n",
    "    return [x, y, vocabulary, vocabulary_inv]\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data) / batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_data = data[shuffle_indices]\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "            \n",
    "x, y, vocabulary, vocabulary_inv=load_data()\n",
    "print type(vocabulary),type(vocabulary_inv)\n",
    "build_predict_data([u'我','love','china'],vocabulary,10)\n",
    "\n",
    "# print jieba.lcut(clean_str(u'我喜欢中国人,.?!，。？！'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from os.path import join, exists, split\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_word2vec(sentence_matrix, vocabulary_inv,\n",
    "                   num_features=300, min_word_count=1, context=10):\n",
    "    \"\"\"\n",
    "    Trains, saves, loads Word2Vec model\n",
    "    Returns initial weights for embedding layer.\n",
    "   \n",
    "    inputs:\n",
    "    sentence_matrix # int matrix: num_sentences x max_sentence_len\n",
    "    vocabulary_inv  # dict {str:int}\n",
    "    num_features    # Word vector dimensionality                      \n",
    "    min_word_count  # Minimum word count                        \n",
    "    context         # Context window size \n",
    "    \"\"\"\n",
    "    model_dir = 'models'\n",
    "    model_name = \"{:d}features_{:d}minwords_{:d}context\".format(num_features, min_word_count, context)\n",
    "    model_name = join(model_dir, model_name)\n",
    "    if exists(model_name):\n",
    "        embedding_model = word2vec.Word2Vec.load(model_name)\n",
    "        print('Load existing Word2Vec model \\'%s\\'' % split(model_name)[-1])\n",
    "    else:\n",
    "        # Set values for various parameters\n",
    "        num_workers = 2  # Number of threads to run in parallel\n",
    "        downsampling = 1e-3  # Downsample setting for frequent words\n",
    "\n",
    "        # Initialize and train the model\n",
    "        print('Training Word2Vec model...')\n",
    "        sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]\n",
    "        embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "                                            size=num_features, min_count=min_word_count,\n",
    "                                            window=context, sample=downsampling)\n",
    "\n",
    "        # If we don't plan to train the model any further, calling \n",
    "        # init_sims will make the model much more memory-efficient.\n",
    "        embedding_model.init_sims(replace=True)\n",
    "\n",
    "        # Saving the model for later use. You can load it later using Word2Vec.load()\n",
    "        if not exists(model_dir):\n",
    "            os.mkdir(model_dir)\n",
    "        print('Saving Word2Vec model \\'%s\\'' % split(model_name)[-1])\n",
    "        embedding_model.save(model_name)\n",
    "\n",
    "    # add unknown words\n",
    "    embedding_weights = [np.array([embedding_model[w] if w in embedding_model\n",
    "                                   else np.random.uniform(-0.25, 0.25, embedding_model.vector_size)\n",
    "                                   for w in vocabulary_inv])]\n",
    "    return embedding_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "('x_train shape:', (873, 345), <type 'numpy.ndarray'>)\n",
      "('x_test shape:', (874, 345), <type 'numpy.ndarray'>)\n",
      "('y_train shape:', (873,), <type 'numpy.ndarray'>)\n",
      "('y_test shape:', (874,), <type 'numpy.ndarray'>)\n",
      "('Model type is', 'CNN-rand')\n",
      "Train on 873 samples, validate on 874 samples\n",
      "Epoch 1/10\n",
      "2s - loss: 0.6868 - acc: 0.5647 - val_loss: 0.6826 - val_acc: 0.5709\n",
      "Epoch 2/10\n",
      "1s - loss: 0.6864 - acc: 0.5578 - val_loss: 0.6809 - val_acc: 0.5709\n",
      "Epoch 3/10\n",
      "1s - loss: 0.6801 - acc: 0.5647 - val_loss: 0.6781 - val_acc: 0.6098\n",
      "Epoch 4/10\n",
      "1s - loss: 0.6717 - acc: 0.5922 - val_loss: 0.6591 - val_acc: 0.6236\n",
      "Epoch 5/10\n",
      "1s - loss: 0.6330 - acc: 0.6449 - val_loss: 0.6183 - val_acc: 0.7140\n",
      "Epoch 6/10\n",
      "1s - loss: 0.5328 - acc: 0.7652 - val_loss: 0.4863 - val_acc: 0.8215\n",
      "Epoch 7/10\n",
      "1s - loss: 0.3729 - acc: 0.8660 - val_loss: 0.3759 - val_acc: 0.8444\n",
      "Epoch 8/10\n",
      "1s - loss: 0.2529 - acc: 0.9061 - val_loss: 0.3502 - val_acc: 0.8764\n",
      "Epoch 9/10\n",
      "2s - loss: 0.1767 - acc: 0.9450 - val_loss: 0.3398 - val_acc: 0.8810\n",
      "Epoch 10/10\n",
      "1s - loss: 0.1365 - acc: 0.9519 - val_loss: 0.3356 - val_acc: 0.8822\n",
      "[[ 0.99703836]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import h5py\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# ---------------------- Parameters section -------------------\n",
    "\n",
    "model_type = \"CNN-rand\"  # CNN-rand|CNN-non-static|CNN-static\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 50\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Prepossessing parameters\n",
    "sequence_length = 400\n",
    "max_words = 5000\n",
    "\n",
    "# Word2Vec parameters (see train_word2vec)\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "#\n",
    "# ---------------------- Parameters end -----------------------\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "print(\"Load data...\")\n",
    "# (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words, start_char=None, oov_char=None,\n",
    "#                                                       index_from=None)\n",
    "\n",
    "# x_train = sequence.pad_sequences(x_train, maxlen=sequence_length, padding=\"post\", truncating=\"post\")\n",
    "# x_test = sequence.pad_sequences(x_test, maxlen=sequence_length, padding=\"post\", truncating=\"post\")\n",
    "# print(\"x_train shape:\", x_train.shape, type(x_train))\n",
    "# print(\"x_test shape:\", x_test.shape, type(x_test))\n",
    "# print(\"y_train shape:\", y_train.shape, type(y_train))\n",
    "# print(\"y_test shape:\", y_test.shape, type(y_test))\n",
    "\n",
    "vocabulary = imdb.get_word_index()\n",
    "vocabulary_inv = dict((v, k) for k, v in vocabulary.items())\n",
    "vocabulary_inv[0] = \"<PAD/>\"\n",
    "\n",
    "# Custom data from dir\n",
    "x, y, vocabulary, vocabulary_inv = load_data()\n",
    "y = np.asarray([temp[1] for temp in y])\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=42)\n",
    "print(\"x_train shape:\", x_train.shape, type(x_train))\n",
    "print(\"x_test shape:\", x_test.shape, type(x_test))\n",
    "print(\"y_train shape:\", y_train.shape, type(y_train))\n",
    "print(\"y_test shape:\", y_test.shape, type(y_test))\n",
    "sequence_length = len(x_train[0])\n",
    "\n",
    "# Prepare embedding layer weights and convert inputs for static model\n",
    "print(\"Model type is\", model_type)\n",
    "if model_type == \"CNN-non-static\" or model_type == \"CNN-static\":\n",
    "    embedding_weights = train_word2vec(np.vstack((x_train, x_test)), vocabulary_inv, num_features=embedding_dim,\n",
    "                                       min_word_count=min_word_count, context=context)\n",
    "if model_type == \"CNN-static\":\n",
    "    x_train = embedding_weights[0][x_train]\n",
    "    x_test = embedding_weights[0][x_test]\n",
    "elif model_type == \"CNN-rand\":\n",
    "    embedding_weights = None\n",
    "else:\n",
    "    raise ValueError(\"Unknown model type\")\n",
    "\n",
    "# Build model\n",
    "input_shape = (sequence_length, embedding_dim) if model_type == \"CNN-static\" else (sequence_length,)\n",
    "model_input = Input(shape=input_shape)\n",
    "\n",
    "# Static model do not have embedding layer\n",
    "if model_type == \"CNN-static\":\n",
    "    z = Dropout(dropout_prob[0])(model_input)\n",
    "else:\n",
    "    z = Embedding(len(vocabulary_inv), embedding_dim, input_length=sequence_length, name=\"embedding\")(model_input)\n",
    "    z = Dropout(dropout_prob[0])(z)\n",
    "\n",
    "# Convolutional block\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(z)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "z = Dropout(dropout_prob[1])(z)\n",
    "z = Dense(hidden_dims, activation=\"relu\")(z)\n",
    "model_output = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Initialize weights with word2vec\n",
    "if model_type == \"CNN-non-static\":\n",
    "    embedding_layer = model.get_layer(\"embedding\")\n",
    "    embedding_layer.set_weights(embedding_weights)\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(x_test, y_test), verbose=2)\n",
    "\n",
    "pos=u'鞋子不错，穿着舒服，很好用'\n",
    "neg=u'假的吧，好臭，鞋不行'\n",
    "print model.predict(build_predict_data(jieba.lcut(pos),vocabulary,sequence_length))\n",
    "\n",
    "model.save('./cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.78081101]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('./cnn.h5')\n",
    "\n",
    "pos=u'鞋子挺漂亮了里面还有一个高科技，感觉挺高的上的然后，外表也挺时尚漂亮'\n",
    "pos=u'鞋子不错，穿着舒服，很好用'\n",
    "pos=u'鞋子很给力，好评'\n",
    "\n",
    "neg=u'质量不好，不是正品'\n",
    "neg=u'鞋子真是不好啊！太廋了！变态硬！'\n",
    "neg=u'垃圾产品。50元不到的做工卖1百多，去那刷那么多好评 我！'\n",
    "\n",
    "print model.predict(build_predict_data(jieba.lcut(pos),vocabulary,sequence_length))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
